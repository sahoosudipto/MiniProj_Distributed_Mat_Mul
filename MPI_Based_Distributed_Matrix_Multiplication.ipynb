{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvypekZ+/fLD48m6fudtcu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahoosudipto/MiniProj_Distributed_Mat_Mul/blob/main/MPI_Based_Distributed_Matrix_Multiplication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fva3B3QOp0uL",
        "outputId": "d58a2540-6fa5-4c1f-dae9-f12031548828"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpi4py\n",
            "  Downloading mpi4py-4.0.1.tar.gz (466 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/466.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m460.8/466.2 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m466.2/466.2 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-4.0.1-cp310-cp310-linux_x86_64.whl size=4266351 sha256=c70c1c40be40760bce2ba45032f743d9b054522526f5082c8e3bed270ef6f3d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/3c/ca/13/13218a83854023ccec184e3af482f0f038b434aa32c19afee8\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-4.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install mpi4py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from mpi4py import MPI\n",
        "\n",
        "def serial_matrix_multiply(A, B):\n",
        "    \"\"\"\n",
        "    Standard serial matrix multiplication implementation\n",
        "\n",
        "    Args:\n",
        "        A (np.ndarray): First input matrix\n",
        "        B (np.ndarray): Second input matrix\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Resulting matrix after multiplication\n",
        "    \"\"\"\n",
        "    rows_A, cols_A = A.shape\n",
        "    rows_B, cols_B = B.shape\n",
        "\n",
        "    if cols_A != rows_B:\n",
        "        raise ValueError(\"Matrix dimensions incompatible for multiplication\")\n",
        "\n",
        "    result = np.zeros((rows_A, cols_B))\n",
        "\n",
        "\n",
        "    for i in range(rows_A):\n",
        "        for j in range(cols_B):\n",
        "            for k in range(cols_A):\n",
        "                result[i][j] += A[i][k] * B[k][j]\n",
        "\n",
        "    return result\n",
        "\n",
        "def distribute_matrix(matrix, comm, root=0):\n",
        "\n",
        "    rank = comm.Get_rank()\n",
        "    size = comm.Get_size()\n",
        "\n",
        "    if rank == root:\n",
        "        matrix_chunks = np.array_split(matrix, size)\n",
        "    else:\n",
        "        matrix_chunks = None\n",
        "\n",
        "    local_chunk = comm.scatter(matrix_chunks, root=root)\n",
        "    return local_chunk\n",
        "\n",
        "def parallel_matrix_multiply(A, B, comm):\n",
        "    rank = comm.Get_rank()\n",
        "    size = comm.Get_size()\n",
        "\n",
        "    # Distribute matrices\n",
        "    local_A = distribute_matrix(A, comm)\n",
        "    local_B = distribute_matrix(B, comm)\n",
        "\n",
        "    # multiplication\n",
        "    local_result = np.dot(local_A, local_B)\n",
        "\n",
        "    # Gather results\n",
        "    global_result = comm.gather(local_result, root=0)\n",
        "\n",
        "    return np.concatenate(global_result) if rank == 0 else None"
      ],
      "metadata": {
        "id": "LS3mjJ_dp5hQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test regular matrix multiplication\n",
        "start_time = time.time()\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "A = np.random.rand(200, 200)\n",
        "B = np.random.rand(200, 200)\n",
        "result = serial_matrix_multiply(A, B)\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(\"Distributed multiplication completed\")\n",
        "print(f\"Execution time: {execution_time} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgRocjgbqElS",
        "outputId": "dfd0d627-832c-4890-a30d-ea61f7f553b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distributed multiplication completed\n",
            "Execution time: 11.247038125991821 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test parallel matrix multiplication\n",
        "start_time = time.time()\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "A = np.random.rand(200, 200)\n",
        "B = np.random.rand(200, 200)\n",
        "result = parallel_matrix_multiply(A, B, comm)\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "if rank == 0:\n",
        "    print(\"Distributed multiplication completed\")\n",
        "    print(f\"Execution time: {execution_time} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFng0Dc5qFbf",
        "outputId": "b928b0a6-7fb5-4891-d55c-2e7ae4252968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distributed multiplication completed\n",
            "Execution time: 0.009556055068969727 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test benchmark matrix multiplication\n",
        "start_time = time.time()\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "A = np.random.rand(1000, 1000)\n",
        "B = np.random.rand(1000, 1000)\n",
        "result = parallel_matrix_multiply(A, B, comm)\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "if rank == 0:\n",
        "  print(\"Distributed multiplication completed\")\n",
        "  print(f\"Execution time: {execution_time} seconds\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ZPEtl_FiqHZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#File structured"
      ],
      "metadata": {
        "id": "_PGzKglQqpn3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D7XSd52Rq3Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## distributed matrix multiplication"
      ],
      "metadata": {
        "id": "PIcLcw22qtLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dist_matrix_mult.py\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "# Function to perform serial matrix multiplication\n",
        "def serial_matrix_mult(A, B):\n",
        "    return np.dot(A, B)\n",
        "\n",
        "# Function to perform distributed matrix multiplication\n",
        "# Function to perform distributed matrix multiplication\n",
        "def distributed_matrix_mult(A, B, m, n, p):\n",
        "    # Determine local rows for each process\n",
        "    local_rows = m // size\n",
        "    start_row = rank * local_rows\n",
        "    end_row = start_row + local_rows\n",
        "\n",
        "    # Scatter matrix A\n",
        "    local_A = np.empty((local_rows, n))\n",
        "    comm.Scatter(A, local_A, root=0)\n",
        "\n",
        "    # Broadcast matrix B\n",
        "    B = comm.bcast(B, root=0)\n",
        "\n",
        "    # Perform local matrix multiplication\n",
        "    local_C = np.dot(local_A, B)\n",
        "\n",
        "    # Gather results\n",
        "    C = np.empty((m, p))\n",
        "    comm.Gather(local_C, C, root=0)\n",
        "\n",
        "    return C\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # ... (same as before) ..."
      ],
      "metadata": {
        "id": "zj5aTgyBq3jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test file"
      ],
      "metadata": {
        "id": "wckD18lzq046"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test_matrix_mult.py\n",
        "import unittest\n",
        "import numpy as np\n",
        "from dist_matrix_mult import serial_matrix_mult, distributed_matrix_mult\n",
        "from mpi4py import MPI\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "class TestMatrixMult(unittest.TestCase):\n",
        "\n",
        "    def test_serial_mult(self):\n",
        "        A = np.array([[1, 2], [3, 4]])\n",
        "        B = np.array([[5, 6], [7, 8]])\n",
        "        C_expected = np.array([[19, 22], [43, 50]])\n",
        "        C_actual = serial_matrix_mult(A, B)\n",
        "        np.testing.assert_array_equal(C_actual, C_expected)\n",
        "\n",
        "    def test_distributed_mult(self):\n",
        "        m = 4\n",
        "        n = 4\n",
        "        p = 4\n",
        "        A = np.random.rand(m, n)\n",
        "        B = np.random.rand(n, p)\n",
        "\n",
        "        if rank == 0:\n",
        "            C_serial = serial_matrix_mult(A, B)\n",
        "\n",
        "        C_dist = distributed_matrix_mult(A, B, m, n, p)\n",
        "\n",
        "        if rank == 0:\n",
        "            np.testing.assert_allclose(C_serial, C_dist, rtol=1e-5, atol=1e-5)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main()"
      ],
      "metadata": {
        "id": "IaX-vd29qhyo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}